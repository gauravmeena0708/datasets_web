<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Metrics Reference - Synthetic Data Evaluation</title>

    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>

    <!-- Chart.js for visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>

    <!-- KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <style>
        /* Custom scrollbar */
        ::-webkit-scrollbar {
            width: 8px;
        }

        ::-webkit-scrollbar-track {
            background: #f1f5f9;
        }

        ::-webkit-scrollbar-thumb {
            background: #cbd5e1;
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: #94a3b8;
        }

        /* Metric card styling */
        .metric-card {
            transition: all 0.3s ease;
        }

        .metric-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.1);
        }

        .metric-card.collapsed .expandable-content {
            display: none;
        }

        /* Tab styling */
        .tab-btn.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        /* Formula box */
        .formula-box {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border-left: 4px solid #667eea;
        }

        /* Range indicator */
        .range-good {
            color: #10b981;
            font-weight: 600;
        }

        .range-bad {
            color: #ef4444;
            font-weight: 600;
        }

        /* Interactive calculator */
        .calculator {
            background: #f8fafc;
            border: 2px dashed #cbd5e1;
        }

        /* Code block */
        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            font-family: 'Courier New', monospace;
            font-size: 0.875rem;
        }

        /* Smooth animations */
        .fade-in {
            animation: fadeIn 0.3s ease-in;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Navigation link styling */
        .nav-link {
            transition: all 0.2s ease;
        }

        .nav-link:hover {
            background: rgba(255, 255, 255, 0.1);
        }

        .nav-link.active {
            background: rgba(255, 255, 255, 0.15);
            border-left: 4px solid #667eea;
        }
    </style>
</head>

<body class="bg-gradient-to-br from-slate-50 to-slate-100 min-h-screen">

    <!-- Header -->
    <header class="bg-gradient-to-r from-indigo-600 via-purple-600 to-pink-600 text-white shadow-lg">
        <div class="container mx-auto px-6 py-6">
            <div class="flex items-center justify-between">
                <div>
                    <h1 class="text-3xl font-bold">Metrics Reference Dashboard</h1>
                    <p class="text-indigo-100 mt-1">Comprehensive guide to synthetic data evaluation metrics</p>
                </div>
                <nav class="hidden md:flex gap-4">
                    <a href="index.html" class="px-4 py-2 bg-white/20 rounded-lg hover:bg-white/30 transition">
                        üè† Dataset Explorer
                    </a>
                    <a href="data.html" class="px-4 py-2 bg-white/30 rounded-lg">
                        üìä Metrics Reference
                    </a>
                </nav>
            </div>
        </div>
    </header>

    <!-- Main Container -->
    <div class="container mx-auto px-4 py-8">

        <!-- Quick Stats -->
        <div class="grid grid-cols-1 md:grid-cols-4 gap-4 mb-8">
            <div class="bg-white p-6 rounded-xl shadow-md border-l-4 border-indigo-500">
                <div class="text-sm text-gray-500 font-medium">Total Metrics</div>
                <div class="text-3xl font-bold text-indigo-600 mt-2">25+</div>
                <div class="text-xs text-gray-400 mt-1">Across 5 categories</div>
            </div>
            <div class="bg-white p-6 rounded-xl shadow-md border-l-4 border-emerald-500">
                <div class="text-sm text-gray-500 font-medium">Statistical</div>
                <div class="text-3xl font-bold text-emerald-600 mt-2">9</div>
                <div class="text-xs text-gray-400 mt-1">Fidelity metrics</div>
            </div>
            <div class="bg-white p-6 rounded-xl shadow-md border-l-4 border-amber-500">
                <div class="text-sm text-gray-500 font-medium">Privacy</div>
                <div class="text-3xl font-bold text-amber-600 mt-2">8</div>
                <div class="text-xs text-gray-400 mt-1">Protection metrics</div>
            </div>
            <div class="bg-white p-6 rounded-xl shadow-md border-l-4 border-blue-500">
                <div class="text-sm text-gray-500 font-medium">ML Efficacy</div>
                <div class="text-3xl font-bold text-blue-600 mt-2">6</div>
                <div class="text-xs text-gray-400 mt-1">Utility metrics</div>
            </div>
        </div>

        <!-- Search Bar -->
        <div class="bg-white p-4 rounded-xl shadow-md mb-6">
            <input type="text" id="metricSearch"
                placeholder="üîç Search metrics (e.g., 'Wasserstein', 'Privacy', 'Accuracy')..."
                class="w-full px-4 py-3 border-2 border-gray-200 rounded-lg focus:border-indigo-500 focus:outline-none text-lg">
        </div>

        <!-- Category Tabs -->
        <div class="bg-white rounded-xl shadow-md p-2 mb-6">
            <div class="flex flex-wrap gap-2">
                <button class="tab-btn active px-6 py-3 rounded-lg font-semibold transition" data-category="all">
                    üìä All Metrics
                </button>
                <button class="tab-btn px-6 py-3 rounded-lg font-semibold transition bg-gray-100 hover:bg-gray-200"
                    data-category="statistical">
                    üìà Statistical Fidelity
                </button>
                <button class="tab-btn px-6 py-3 rounded-lg font-semibold transition bg-gray-100 hover:bg-gray-200"
                    data-category="ml">
                    ü§ñ ML Efficacy
                </button>
                <button class="tab-btn px-6 py-3 rounded-lg font-semibold transition bg-gray-100 hover:bg-gray-200"
                    data-category="privacy">
                    üîí Privacy
                </button>
                <button class="tab-btn px-6 py-3 rounded-lg font-semibold transition bg-gray-100 hover:bg-gray-200"
                    data-category="coverage">
                    üì¶ Coverage
                </button>
            </div>
        </div>

        <!-- Metrics Container -->
        <div id="metricsContainer" class="space-y-6">
            <!-- Metrics will be injected here by JavaScript -->
        </div>

    </div>

    <!-- Footer -->
    <footer class="bg-gray-800 text-white mt-16 py-8">
        <div class="container mx-auto px-6 text-center">
            <p class="text-gray-400">Metrics Reference Dashboard | Based on sdeval and research papers</p>
            <p class="text-gray-500 text-sm mt-2">For research and educational purposes</p>
        </div>
    </footer>

    <script>
        // Metrics data structure
        const metricsData = {
            statistical: [
                {
                    name: "Alpha Precision (Œ±)",
                    symbol: "\\alpha",
                    category: "statistical",
                    subcategory: "Categorical",
                    formula: "\\alpha = \\frac{|S_{syn} \\cap S_{real}|}{|S_{syn}|}",
                    description: "Measures the fraction of synthetic categorical values that exist in the real data.",
                    interpretation: "Higher values indicate better quality - synthetic data only contains realistic categories.",
                    range: "[0, 1]",
                    bestValue: "1.0 (all synthetic values are realistic)",
                    worstValue: "0.0 (no synthetic values match real data)",
                    code: `from sdeval.metrics.statistical import compute_alpha_precision

alpha = compute_alpha_precision(
    real_df=real_data,
    synthetic_df=synthetic_data,
    categorical_columns=['gender', 'education', 'occupation']
)
print(f"Alpha Precision: {alpha:.3f}")`,
                    example: {
                        real: ["A", "B", "C"],
                        synthetic: ["A", "B", "D"],
                        result: "Œ± = 2/3 = 0.667 (2 out of 3 synthetic values exist in real)"
                    }
                },
                {
                    name: "Beta Recall (Œ≤)",
                    symbol: "\\beta",
                    category: "statistical",
                    subcategory: "Categorical",
                    formula: "\\beta = \\frac{|S_{syn} \\cap S_{real}|}{|S_{real}|}",
                    description: "Measures the fraction of real categorical values that are covered by the synthetic data.",
                    interpretation: "Higher values indicate better coverage - synthetic data represents all real categories.",
                    range: "[0, 1]",
                    bestValue: "1.0 (all real categories are represented)",
                    worstValue: "0.0 (no real categories are covered)",
                    code: `from sdeval.metrics.statistical import compute_beta_recall

beta = compute_beta_recall(
    real_df=real_data,
    synthetic_df=synthetic_data,
    categorical_columns=['gender', 'education', 'occupation']
)
print(f"Beta Recall: {beta:.3f}")`,
                    example: {
                        real: ["A", "B", "C"],
                        synthetic: ["A", "B"],
                        result: "Œ≤ = 2/3 = 0.667 (2 out of 3 real values are covered)"
                    }
                },
                {
                    name: "Wasserstein Distance",
                    symbol: "W",
                    category: "statistical",
                    subcategory: "Numerical",
                    formula: "W(P, Q) = \\inf_{\\gamma \\in \\Gamma(P,Q)} \\mathbb{E}_{(x,y)\\sim\\gamma}[||x-y||]",
                    description: "Also known as Earth Mover's Distance. Measures the minimum cost to transform one distribution into another.",
                    interpretation: "Lower values indicate distributions are more similar. Sensitive to both location and shape differences.",
                    range: "[0, ‚àû)",
                    bestValue: "0.0 (identical distributions)",
                    worstValue: "Higher values indicate greater dissimilarity",
                    code: `from sdeval.metrics.statistical import compute_wasserstein_distance

wasserstein = compute_wasserstein_distance(
    real_df=real_data,
    synthetic_df=synthetic_data,
    numerical_columns=['age', 'income', 'hours_per_week']
)
print(f"Wasserstein Distance: {wasserstein:.4f}")`,
                    example: {
                        interpretation: "If W=0.15 for 'age' column, synthetic ages need to 'move' 0.15 units on average to match real distribution"
                    }
                },
                {
                    name: "Kolmogorov-Smirnov (KS) Statistic",
                    symbol: "D_{KS}",
                    category: "statistical",
                    subcategory: "Numerical",
                    formula: "D_{KS} = \\sup_x |F_{real}(x) - F_{syn}(x)|",
                    description: "Maximum absolute difference between cumulative distribution functions (CDFs) of real and synthetic data.",
                    interpretation: "Lower values indicate more similar distributions. Non-parametric test that's sensitive to any differences.",
                    range: "[0, 1]",
                    bestValue: "0.0 (identical CDFs)",
                    worstValue: "1.0 (completely different distributions)",
                    code: `from scipy.stats import ks_2samp

# For each numerical column
ks_stat, p_value = ks_2samp(
    real_data['age'],
    synthetic_data['age']
)
print(f"KS Statistic: {ks_stat:.4f}, p-value: {p_value:.4f}")`,
                    example: {
                        interpretation: "KS=0.05 with p>0.05 suggests distributions are statistically similar"
                    }
                },
                {
                    name: "Jensen-Shannon Divergence (JSD)",
                    symbol: "JSD",
                    category: "statistical",
                    subcategory: "Numerical",
                    formula: "JSD(P||Q) = \\frac{1}{2}D_{KL}(P||M) + \\frac{1}{2}D_{KL}(Q||M), \\quad M = \\frac{1}{2}(P+Q)",
                    description: "Symmetric and smoothed version of KL divergence. Measures similarity between probability distributions.",
                    interpretation: "Lower values indicate more similar distributions. Bounded and symmetric unlike KL divergence.",
                    range: "[0, 1]",
                    bestValue: "0.0 (identical distributions)",
                    worstValue: "1.0 (completely different distributions)",
                    code: `from scipy.spatial.distance import jensenshannon
import numpy as np

# Create histograms
real_hist, bins = np.histogram(real_data['age'], bins=50, density=True)
syn_hist, _ = np.histogram(synthetic_data['age'], bins=bins, density=True)

# Compute JSD
jsd = jensenshannon(real_hist, syn_hist, base=2.0) ** 2
print(f"Jensen-Shannon Divergence: {jsd:.4f}")`,
                    example: {
                        interpretation: "JSD=0.02 indicates very similar distributions (< 0.1 is generally good)"
                    }
                },
                {
                    name: "Mean Absolute Difference",
                    symbol: "MAD",
                    category: "statistical",
                    subcategory: "Numerical",
                    formula: "MAD = \\frac{1}{n}\\sum_{i=1}^{n}|\\mu_{real}^{(i)} - \\mu_{syn}^{(i)}|",
                    description: "Average absolute difference between column means of real and synthetic data.",
                    interpretation: "Lower values indicate synthetic data preserves the central tendency of real data.",
                    range: "[0, ‚àû)",
                    bestValue: "0.0 (identical means)",
                    worstValue: "Higher values indicate larger discrepancies",
                    code: `from sdeval.metrics.statistical import compute_mean_absolute_difference

mad = compute_mean_absolute_difference(
    real_df=real_data,
    synthetic_df=synthetic_data,
    numerical_columns=['age', 'income', 'hours_per_week']
)
print(f"Mean Absolute Difference: {mad:.4f}")`,
                    example: {
                        calculation: "If real mean age=38.5 and synthetic mean age=38.2, contribution is |38.5-38.2|=0.3"
                    }
                },
                {
                    name: "Std Absolute Difference",
                    symbol: "SAD",
                    category: "statistical",
                    subcategory: "Numerical",
                    formula: "SAD = \\frac{1}{n}\\sum_{i=1}^{n}|\\sigma_{real}^{(i)} - \\sigma_{syn}^{(i)}|",
                    description: "Average absolute difference between column standard deviations of real and synthetic data.",
                    interpretation: "Lower values indicate synthetic data preserves the spread/variability of real data.",
                    range: "[0, ‚àû)",
                    bestValue: "0.0 (identical standard deviations)",
                    worstValue: "Higher values indicate larger discrepancies in variability",
                    code: `from sdeval.metrics.statistical import compute_std_absolute_difference

sad = compute_std_absolute_difference(
    real_df=real_data,
    synthetic_df=synthetic_data,
    numerical_columns=['age', 'income', 'hours_per_week']
)
print(f"Std Absolute Difference: {sad:.4f}")`,
                    example: {
                        calculation: "If real std=12.5 and synthetic std=12.8, contribution is |12.5-12.8|=0.3"
                    }
                },
                {
                    name: "Correlation Delta (Frobenius)",
                    symbol: "||\\Delta C||_F",
                    category: "statistical",
                    subcategory: "Numerical",
                    formula: "||\\Delta C||_F = ||C_{real} - C_{syn}||_F = \\sqrt{\\sum_{i,j}(C_{real}^{ij} - C_{syn}^{ij})^2}",
                    description: "Frobenius norm of the difference between correlation matrices. Measures how well synthetic data preserves feature relationships.",
                    interpretation: "Lower values indicate better preservation of correlations between features.",
                    range: "[0, ‚àû)",
                    bestValue: "0.0 (identical correlation structures)",
                    worstValue: "Higher values indicate different correlation patterns",
                    code: `import numpy as np

# Compute correlation matrices
real_corr = real_data[numerical_cols].corr()
syn_corr = synthetic_data[numerical_cols].corr()

# Frobenius norm of difference
corr_delta = np.linalg.norm(real_corr - syn_corr, ord='fro')
print(f"Correlation Delta: {corr_delta:.4f}")`,
                    example: {
                        interpretation: "If delta=0.5 for 5 features, correlation structure is reasonably preserved"
                    }
                },
                {
                    name: "Chi-Square Test",
                    symbol: "\\chi^2",
                    category: "statistical",
                    subcategory: "Categorical",
                    formula: "\\chi^2 = \\sum_{i=1}^{k}\\frac{(O_i - E_i)^2}{E_i}",
                    description: "Statistical test comparing observed (synthetic) vs expected (real) frequencies for categorical variables.",
                    interpretation: "Higher p-values (>0.05) indicate distributions are statistically similar. Lower chi-square values are better.",
                    range: "[0, ‚àû)",
                    bestValue: "Low œá¬≤ with high p-value (distributions match)",
                    worstValue: "High œá¬≤ with low p-value (distributions differ significantly)",
                    code: `from scipy.stats import chisquare

# Get value counts
real_counts = real_data['education'].value_counts()
syn_counts = synthetic_data['education'].value_counts()

# Align categories
categories = real_counts.index.union(syn_counts.index)
obs = syn_counts.reindex(categories, fill_value=0)
exp = real_counts.reindex(categories, fill_value=0)

# Normalize expected to match observed total
exp = exp / exp.sum() * obs.sum()

chi2_stat, p_value = chisquare(f_obs=obs, f_exp=exp)
print(f"Chi-Square: {chi2_stat:.2f}, p-value: {p_value:.4f}")`,
                    example: {
                        interpretation: "œá¬≤=5.2, p=0.27 suggests synthetic categorical distribution matches real data well"
                    }
                }
            ],
            ml: [
                {
                    name: "TSTR Accuracy",
                    symbol: "Acc_{TSTR}",
                    category: "ml",
                    subcategory: "Classification",
                    formula: "Acc = \\frac{TP + TN}{TP + TN + FP + FN}",
                    description: "Train on Synthetic, Test on Real. Measures if a model trained on synthetic data can predict real data accurately.",
                    interpretation: "Higher accuracy indicates synthetic data captures the predictive patterns of real data. Compare with TRTR (train/test on real) as baseline.",
                    range: "[0, 1]",
                    bestValue: "Close to TRTR accuracy (synthetic is as useful as real)",
                    worstValue: "Much lower than TRTR (synthetic data is not useful)",
                    code: `from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Train on synthetic
model = RandomForestClassifier(n_estimators=100, random_state=42)
X_syn = synthetic_data.drop('target', axis=1)
y_syn = synthetic_data['target']
model.fit(X_syn, y_syn)

# Test on real
X_real = real_data.drop('target', axis=1)
y_real = real_data['target']
predictions = model.predict(X_real)

tstr_accuracy = accuracy_score(y_real, predictions)
print(f"TSTR Accuracy: {tstr_accuracy:.3f}")`,
                    example: {
                        interpretation: "If TRTR=0.85 and TSTR=0.82, synthetic data is 96% as useful (good quality)"
                    }
                },
                {
                    name: "F1 Score (Macro)",
                    symbol: "F1_{macro}",
                    category: "ml",
                    subcategory: "Classification",
                    formula: "F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}, \\quad F1_{macro} = \\frac{1}{K}\\sum_{k=1}^{K}F1_k",
                    description: "Harmonic mean of precision and recall, averaged across all classes. Useful for imbalanced datasets.",
                    interpretation: "Higher F1 indicates better balance between precision and recall. Macro averaging treats all classes equally.",
                    range: "[0, 1]",
                    bestValue: "1.0 (perfect precision and recall)",
                    worstValue: "0.0 (no correct predictions)",
                    code: `from sklearn.metrics import f1_score

# After training model on synthetic and testing on real
f1_macro = f1_score(y_real, predictions, average='macro')
print(f"F1 Score (Macro): {f1_macro:.3f}")`,
                    example: {
                        interpretation: "F1=0.78 indicates reasonably balanced performance across all classes"
                    }
                },
                {
                    name: "RMSE (Root Mean Squared Error)",
                    symbol: "RMSE",
                    category: "ml",
                    subcategory: "Regression",
                    formula: "RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}",
                    description: "Square root of average squared prediction errors. Penalizes large errors more than small ones.",
                    interpretation: "Lower RMSE indicates better predictions. In same units as target variable.",
                    range: "[0, ‚àû)",
                    bestValue: "0.0 (perfect predictions)",
                    worstValue: "Higher values indicate larger prediction errors",
                    code: `from sklearn.metrics import mean_squared_error
import numpy as np

# After training regressor on synthetic and testing on real
rmse = np.sqrt(mean_squared_error(y_real, predictions))
print(f"RMSE: {rmse:.4f}")`,
                    example: {
                        interpretation: "If predicting age, RMSE=5.2 means predictions are off by ~5.2 years on average"
                    }
                },
                {
                    name: "MAE (Mean Absolute Error)",
                    symbol: "MAE",
                    category: "ml",
                    subcategory: "Regression",
                    formula: "MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|",
                    description: "Average absolute difference between predictions and actual values. More robust to outliers than RMSE.",
                    interpretation: "Lower MAE indicates better predictions. Easier to interpret than RMSE.",
                    range: "[0, ‚àû)",
                    bestValue: "0.0 (perfect predictions)",
                    worstValue: "Higher values indicate larger prediction errors",
                    code: `from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(y_real, predictions)
print(f"MAE: {mae:.4f}")`,
                    example: {
                        interpretation: "MAE=3.8 for age prediction means average error is 3.8 years"
                    }
                },
                {
                    name: "R¬≤ Score (Coefficient of Determination)",
                    symbol: "R^2",
                    category: "ml",
                    subcategory: "Regression",
                    formula: "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}",
                    description: "Proportion of variance in the target variable explained by the model. Indicates goodness of fit.",
                    interpretation: "Higher R¬≤ indicates better fit. Can be negative if model is worse than predicting the mean.",
                    range: "(-‚àû, 1]",
                    bestValue: "1.0 (perfect fit)",
                    worstValue: "Negative values (worse than baseline)",
                    code: `from sklearn.metrics import r2_score

r2 = r2_score(y_real, predictions)
print(f"R¬≤ Score: {r2:.4f}")`,
                    example: {
                        interpretation: "R¬≤=0.75 means model explains 75% of variance in target variable"
                    }
                }
            ],
            privacy: [
                {
                    name: "DCR (Distance to Closest Record)",
                    symbol: "DCR_t",
                    category: "privacy",
                    subcategory: "Distance-Based",
                    formula: "DCR_t = \\frac{1}{|S_{syn}|}\\sum_{s \\in S_{syn}} \\mathbb{1}[\\min_{r \\in S_{real}} d(s,r) < t]",
                    description: "Fraction of synthetic records that are suspiciously close to real records (within threshold t). Indicates potential privacy leakage.",
                    interpretation: "Lower DCR is better for privacy. Common thresholds: 1e-8, 1e-6, 1e-4, 1e-2.",
                    range: "[0, 1]",
                    bestValue: "0.0 (no synthetic records too close to real)",
                    worstValue: "1.0 (all synthetic records are copies of real)",
                    code: `from sdeval.metrics.privacy import compute_dcr

dcr = compute_dcr(
    real_df=real_data,
    synthetic_df=synthetic_data,
    numerical_columns=['age', 'income', 'hours_per_week'],
    threshold=1e-6
)
print(f"DCR at 1e-6: {dcr:.4f}")`,
                    example: {
                        interpretation: "DCR=0.02 at threshold 1e-6 means 2% of synthetic records are very close to real (potential privacy risk)"
                    }
                },
                {
                    name: "NNDR (Nearest Neighbor Distance Ratio)",
                    symbol: "NNDR",
                    category: "privacy",
                    subcategory: "Distance-Based",
                    formula: "NNDR = \\frac{d_1}{d_2}",
                    description: "Ratio of distance to nearest real neighbor vs second-nearest. Lower values suggest synthetic record is suspiciously similar to one specific real record.",
                    interpretation: "Higher NNDR is better for privacy. Values close to 1.0 indicate synthetic record is equidistant from multiple real records (good).",
                    range: "[0, 1]",
                    bestValue: "~1.0 (equidistant from multiple real records)",
                    worstValue: "~0.0 (very close to one specific real record)",
                    code: `from sdeval.metrics.privacy import compute_nndr

nndr = compute_nndr(
    real_df=real_data,
    synthetic_df=synthetic_data,
    numerical_columns=['age', 'income', 'hours_per_week']
)
print(f"NNDR: {nndr:.4f}")`,
                    example: {
                        interpretation: "NNDR=0.85 suggests synthetic records are reasonably distributed (good privacy)"
                    }
                },
                {
                    name: "Distance Percentiles",
                    symbol: "d_{p50}, d_{p95}",
                    category: "privacy",
                    subcategory: "Distance-Based",
                    formula: "d_{p} = \\text{percentile}_p(\\{\\min_{r \\in S_{real}} d(s,r) : s \\in S_{syn}\\})",
                    description: "Distribution of nearest neighbor distances. Higher percentiles indicate better privacy protection.",
                    interpretation: "Higher distances are better. p50 (median) and p95 are commonly reported.",
                    range: "[0, ‚àû)",
                    bestValue: "Higher values (synthetic records are far from real)",
                    worstValue: "Lower values (synthetic records are close to real)",
                    code: `import numpy as np
from sklearn.neighbors import NearestNeighbors

# Standardize and find nearest neighbors
nbrs = NearestNeighbors(n_neighbors=1).fit(real_data_scaled)
distances, _ = nbrs.kneighbors(synthetic_data_scaled)

p50 = np.percentile(distances, 50)
p95 = np.percentile(distances, 95)
print(f"Distance p50: {p50:.4f}, p95: {p95:.4f}")`,
                    example: {
                        interpretation: "p50=1.2, p95=2.8 indicates most synthetic records are reasonably distant from real"
                    }
                },
                {
                    name: "MIA (Membership Inference Attack)",
                    symbol: "Acc_{MIA}",
                    category: "privacy",
                    subcategory: "Attack-Based",
                    formula: "Acc_{MIA} = \\frac{TP + TN}{TP + TN + FP + FN}",
                    description: "Train a classifier to distinguish real from synthetic records. Lower accuracy means better privacy (harder to distinguish).",
                    interpretation: "Accuracy close to 0.5 (random guessing) is ideal. Higher accuracy indicates privacy leakage.",
                    range: "[0, 1]",
                    bestValue: "~0.5 (cannot distinguish real from synthetic)",
                    worstValue: "~1.0 (can perfectly identify real records)",
                    code: `from sdeval.metrics.privacy import compute_membership_inference_metrics

mia_results = compute_membership_inference_metrics(
    real_df=real_data,
    synthetic_df=synthetic_data,
    sample_size=1000,
    random_state=42
)
print(f"MIA Accuracy: {mia_results['privacy_mia_accuracy']:.3f}")
print(f"MIA AUC: {mia_results['privacy_mia_auc']:.3f}")`,
                    example: {
                        interpretation: "MIA Accuracy=0.58 (close to 0.5) suggests good privacy protection"
                    }
                },
                {
                    name: "K-Anonymity",
                    symbol: "k",
                    category: "privacy",
                    subcategory: "Formal Privacy",
                    formula: "k = \\min_{g \\in G} |g|",
                    description: "Minimum group size when grouping by quasi-identifiers. Each record is indistinguishable from at least k-1 others.",
                    interpretation: "Higher k provides stronger privacy. Typical values: k=5 or k=10.",
                    range: "[1, ‚àû)",
                    bestValue: "Higher k (stronger privacy)",
                    worstValue: "k=1 (no privacy protection)",
                    code: `from sdeval.metrics.privacy import compute_k_anonymity_metrics

k_anon = compute_k_anonymity_metrics(
    synthetic_df=synthetic_data,
    quasi_identifiers=['age', 'gender', 'zipcode'],
    k=5
)
print(f"Min group size: {k_anon['privacy_k_anonymity_min_group']}")
print(f"Violating groups: {k_anon['privacy_k_anonymity_violating_groups_ratio']:.2%}")`,
                    example: {
                        interpretation: "k=5 with 0% violating groups means every record is in a group of at least 5 similar records"
                    }
                },
                {
                    name: "Differential Privacy (Œµ, Œ¥)",
                    symbol: "\\varepsilon, \\delta",
                    category: "privacy",
                    subcategory: "Formal Privacy",
                    formula: "\\Pr[M(D) \\in S] \\leq e^{\\varepsilon} \\Pr[M(D') \\in S] + \\delta",
                    description: "Formal privacy guarantee. Œµ (epsilon) is privacy budget, Œ¥ (delta) is failure probability. Lower values provide stronger privacy.",
                    interpretation: "Œµ < 1 is strong privacy, Œµ < 10 is acceptable. Œ¥ should be very small (< 1/n).",
                    range: "Œµ ‚àà [0, ‚àû), Œ¥ ‚àà [0, 1]",
                    bestValue: "Œµ ‚Üí 0, Œ¥ ‚Üí 0 (strongest privacy)",
                    worstValue: "Large Œµ (weak privacy)",
                    code: `# Differential privacy is typically enforced during training
# Example with Opacus (PyTorch DP library)
from opacus import PrivacyEngine

privacy_engine = PrivacyEngine()
model, optimizer, data_loader = privacy_engine.make_private(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    noise_multiplier=1.1,
    max_grad_norm=1.0,
)

# After training, get privacy spent
epsilon = privacy_engine.get_epsilon(delta=1e-5)
print(f"Privacy spent: Œµ={epsilon:.2f} at Œ¥=1e-5")`,
                    example: {
                        interpretation: "Œµ=3.0, Œ¥=1e-5 provides reasonable privacy for most applications"
                    }
                }
            ],
            coverage: [
                {
                    name: "Feature Coverage",
                    symbol: "C_{feat}",
                    category: "coverage",
                    subcategory: "Completeness",
                    formula: "C_{feat} = \\frac{|\\{f : f \\text{ has valid synthetic data}\\}|}{|F_{total}|}",
                    description: "Fraction of features that have valid (non-null, non-zero) synthetic data.",
                    interpretation: "Higher coverage indicates synthetic data is more complete. Should be close to 1.0.",
                    range: "[0, 1]",
                    bestValue: "1.0 (all features have valid data)",
                    worstValue: "0.0 (no features have valid data)",
                    code: `# Calculate feature coverage
total_features = len(real_data.columns)
valid_features = sum(
    synthetic_data[col].notna().any() and 
    (synthetic_data[col] != 0).any()
    for col in synthetic_data.columns
)
feature_coverage = valid_features / total_features
print(f"Feature Coverage: {feature_coverage:.2%}")`,
                    example: {
                        interpretation: "Coverage=0.95 means 95% of features have valid synthetic values"
                    }
                },
                {
                    name: "Range Coverage",
                    symbol: "C_{range}",
                    category: "coverage",
                    subcategory: "Completeness",
                    formula: "C_{range} = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\max(S_{syn}^i) - \\min(S_{syn}^i)}{\\max(S_{real}^i) - \\min(S_{real}^i)}",
                    description: "Fraction of real data range covered by synthetic data for numerical features.",
                    interpretation: "Values close to 1.0 indicate synthetic data spans the full range of real data.",
                    range: "[0, ‚àû)",
                    bestValue: "~1.0 (synthetic covers full real range)",
                    worstValue: "0.0 (synthetic has no range) or >> 1.0 (synthetic exceeds real range)",
                    code: `# Calculate range coverage for numerical columns
range_coverages = []
for col in numerical_columns:
    real_range = real_data[col].max() - real_data[col].min()
    syn_range = synthetic_data[col].max() - synthetic_data[col].min()
    if real_range > 0:
        range_coverages.append(syn_range / real_range)

avg_range_coverage = np.mean(range_coverages)
print(f"Range Coverage: {avg_range_coverage:.2%}")`,
                    example: {
                        interpretation: "Coverage=0.92 means synthetic data covers 92% of the real data's range"
                    }
                },
                {
                    name: "Category Coverage",
                    symbol: "C_{cat}",
                    category: "coverage",
                    subcategory: "Completeness",
                    formula: "C_{cat} = \\frac{1}{m}\\sum_{j=1}^{m}\\frac{|\\text{categories}_{syn}^j \\cap \\text{categories}_{real}^j|}{|\\text{categories}_{real}^j|}",
                    description: "Fraction of real categorical values that appear in synthetic data. Similar to Beta Recall but averaged.",
                    interpretation: "Higher values indicate synthetic data represents all real categories well.",
                    range: "[0, 1]",
                    bestValue: "1.0 (all real categories present in synthetic)",
                    worstValue: "0.0 (no real categories in synthetic)",
                    code: `# Calculate category coverage
category_coverages = []
for col in categorical_columns:
    real_cats = set(real_data[col].unique())
    syn_cats = set(synthetic_data[col].unique())
    if len(real_cats) > 0:
        coverage = len(real_cats & syn_cats) / len(real_cats)
        category_coverages.append(coverage)

avg_cat_coverage = np.mean(category_coverages)
print(f"Category Coverage: {avg_cat_coverage:.2%}")`,
                    example: {
                        interpretation: "Coverage=0.88 means 88% of real categories are represented in synthetic data"
                    }
                }
            ]
        };

        // Render metrics function
        function renderMetrics(category = 'all', searchTerm = '') {
            const container = document.getElementById('metricsContainer');
            container.innerHTML = '';

            let allMetrics = [];
            if (category === 'all') {
                Object.values(metricsData).forEach(categoryMetrics => {
                    allMetrics = allMetrics.concat(categoryMetrics);
                });
            } else {
                allMetrics = metricsData[category] || [];
            }

            // Filter by search term
            if (searchTerm) {
                const term = searchTerm.toLowerCase();
                allMetrics = allMetrics.filter(metric =>
                    metric.name.toLowerCase().includes(term) ||
                    metric.description.toLowerCase().includes(term) ||
                    metric.subcategory.toLowerCase().includes(term)
                );
            }

            if (allMetrics.length === 0) {
                container.innerHTML = '<div class="text-center text-gray-500 py-12">No metrics found matching your search.</div>';
                return;
            }

            allMetrics.forEach((metric, index) => {
                const card = createMetricCard(metric, index);
                container.appendChild(card);
            });

            // Render LaTeX
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false },
                    { left: "\\(", right: "\\)", display: false },
                    { left: "\\[", right: "\\]", display: true }
                ]
            });
        }

        // Create metric card
        function createMetricCard(metric, index) {
            const card = document.createElement('div');
            card.className = 'metric-card bg-white rounded-xl shadow-md p-6 fade-in';
            card.style.animationDelay = `${index * 0.05}s`;

            const categoryColors = {
                statistical: 'bg-emerald-100 text-emerald-800',
                ml: 'bg-blue-100 text-blue-800',
                privacy: 'bg-amber-100 text-amber-800',
                coverage: 'bg-purple-100 text-purple-800'
            };

            card.innerHTML = `
                <div class="flex items-start justify-between mb-4">
                    <div class="flex-1">
                        <div class="flex items-center gap-3 mb-2">
                            <h3 class="text-2xl font-bold text-gray-900">${metric.name}</h3>
                            <span class="px-3 py-1 rounded-full text-xs font-semibold ${categoryColors[metric.category]}">
                                ${metric.subcategory}
                            </span>
                        </div>
                        <p class="text-gray-600">${metric.description}</p>
                    </div>
                    <button onclick="toggleCard(this)" class="ml-4 text-gray-400 hover:text-gray-600 transition">
                        <svg class="w-6 h-6 transform transition-transform" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
                        </svg>
                    </button>
                </div>

                <div class="expandable-content">
                    <!-- Formula -->
                    <div class="formula-box p-4 rounded-lg mb-4">
                        <div class="text-sm font-semibold text-gray-700 mb-2">Mathematical Formula:</div>
                        <div class="text-center text-lg">$$${metric.formula}$$</div>
                    </div>

                    <!-- Interpretation -->
                    <div class="mb-4">
                        <div class="text-sm font-semibold text-gray-700 mb-2">üìä Interpretation:</div>
                        <p class="text-gray-600">${metric.interpretation}</p>
                    </div>

                    <!-- Range -->
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-4">
                        <div class="bg-gray-50 p-3 rounded-lg">
                            <div class="text-xs text-gray-500 mb-1">Range</div>
                            <div class="font-mono font-semibold">${metric.range}</div>
                        </div>
                        <div class="bg-green-50 p-3 rounded-lg">
                            <div class="text-xs text-gray-500 mb-1">Best Value</div>
                            <div class="font-semibold text-green-700">${metric.bestValue}</div>
                        </div>
                        <div class="bg-red-50 p-3 rounded-lg">
                            <div class="text-xs text-gray-500 mb-1">Worst Value</div>
                            <div class="font-semibold text-red-700">${metric.worstValue}</div>
                        </div>
                    </div>

                    <!-- Code Example -->
                    <div class="mb-4">
                        <div class="flex items-center justify-between mb-2">
                            <div class="text-sm font-semibold text-gray-700">üíª Code Example:</div>
                            <button onclick="copyCode(this)" class="text-xs px-3 py-1 bg-gray-200 hover:bg-gray-300 rounded transition">
                                Copy
                            </button>
                        </div>
                        <pre class="code-block p-4 rounded-lg overflow-x-auto"><code>${escapeHtml(metric.code)}</code></pre>
                    </div>

                    <!-- Example -->
                    ${metric.example ? `
                    <div class="bg-indigo-50 p-4 rounded-lg">
                        <div class="text-sm font-semibold text-indigo-900 mb-2">üí° Example:</div>
                        <div class="text-sm text-indigo-800">
                            ${metric.example.real ? `<div><strong>Real:</strong> ${JSON.stringify(metric.example.real)}</div>` : ''}
                            ${metric.example.synthetic ? `<div><strong>Synthetic:</strong> ${JSON.stringify(metric.example.synthetic)}</div>` : ''}
                            ${metric.example.result ? `<div class="mt-2"><strong>Result:</strong> ${metric.example.result}</div>` : ''}
                            ${metric.example.interpretation ? `<div class="mt-2">${metric.example.interpretation}</div>` : ''}
                            ${metric.example.calculation ? `<div class="mt-2">${metric.example.calculation}</div>` : ''}
                        </div>
                    </div>
                    ` : ''}
                </div>
            `;

            return card;
        }

        // Toggle card expansion
        function toggleCard(button) {
            const card = button.closest('.metric-card');
            const svg = button.querySelector('svg');
            card.classList.toggle('collapsed');
            svg.classList.toggle('rotate-180');
        }

        // Copy code to clipboard
        function copyCode(button) {
            const code = button.parentElement.nextElementSibling.textContent;
            navigator.clipboard.writeText(code).then(() => {
                button.textContent = 'Copied!';
                setTimeout(() => button.textContent = 'Copy', 2000);
            });
        }

        // Escape HTML
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        // Tab switching
        document.querySelectorAll('.tab-btn').forEach(btn => {
            btn.addEventListener('click', () => {
                document.querySelectorAll('.tab-btn').forEach(b => {
                    b.classList.remove('active');
                    b.classList.add('bg-gray-100', 'hover:bg-gray-200');
                });
                btn.classList.add('active');
                btn.classList.remove('bg-gray-100', 'hover:bg-gray-200');

                const category = btn.dataset.category;
                const searchTerm = document.getElementById('metricSearch').value;
                renderMetrics(category, searchTerm);
            });
        });

        // Search functionality
        document.getElementById('metricSearch').addEventListener('input', (e) => {
            const activeTab = document.querySelector('.tab-btn.active');
            const category = activeTab.dataset.category;
            renderMetrics(category, e.target.value);
        });

        // Initial render
        renderMetrics('all');
    </script>
</body>

</html>